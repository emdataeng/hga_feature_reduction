{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "at7Y4NYQwO6j",
        "outputId": "240b1393-9fcc-48d9-9502-a4005d3609f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gdown in d:\\users\\dario\\onedrive\\uu\\courses\\ncml\\project ncml\\hga_feature_selection\\.venv\\lib\\site-packages (5.2.0)\n",
            "Requirement already satisfied: pandas in d:\\users\\dario\\onedrive\\uu\\courses\\ncml\\project ncml\\hga_feature_selection\\.venv\\lib\\site-packages (2.2.3)\n",
            "Requirement already satisfied: numpy in d:\\users\\dario\\onedrive\\uu\\courses\\ncml\\project ncml\\hga_feature_selection\\.venv\\lib\\site-packages (2.2.5)\n",
            "Requirement already satisfied: scikit-learn in d:\\users\\dario\\onedrive\\uu\\courses\\ncml\\project ncml\\hga_feature_selection\\.venv\\lib\\site-packages (1.6.1)\n",
            "Requirement already satisfied: matplotlib in d:\\users\\dario\\onedrive\\uu\\courses\\ncml\\project ncml\\hga_feature_selection\\.venv\\lib\\site-packages (3.10.1)\n",
            "Requirement already satisfied: seaborn in d:\\users\\dario\\onedrive\\uu\\courses\\ncml\\project ncml\\hga_feature_selection\\.venv\\lib\\site-packages (0.13.2)\n",
            "Requirement already satisfied: beautifulsoup4 in d:\\users\\dario\\onedrive\\uu\\courses\\ncml\\project ncml\\hga_feature_selection\\.venv\\lib\\site-packages (from gdown) (4.13.4)\n",
            "Requirement already satisfied: filelock in d:\\users\\dario\\onedrive\\uu\\courses\\ncml\\project ncml\\hga_feature_selection\\.venv\\lib\\site-packages (from gdown) (3.18.0)\n",
            "Requirement already satisfied: requests[socks] in d:\\users\\dario\\onedrive\\uu\\courses\\ncml\\project ncml\\hga_feature_selection\\.venv\\lib\\site-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in d:\\users\\dario\\onedrive\\uu\\courses\\ncml\\project ncml\\hga_feature_selection\\.venv\\lib\\site-packages (from gdown) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\users\\dario\\onedrive\\uu\\courses\\ncml\\project ncml\\hga_feature_selection\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in d:\\users\\dario\\onedrive\\uu\\courses\\ncml\\project ncml\\hga_feature_selection\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in d:\\users\\dario\\onedrive\\uu\\courses\\ncml\\project ncml\\hga_feature_selection\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in d:\\users\\dario\\onedrive\\uu\\courses\\ncml\\project ncml\\hga_feature_selection\\.venv\\lib\\site-packages (from scikit-learn) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in d:\\users\\dario\\onedrive\\uu\\courses\\ncml\\project ncml\\hga_feature_selection\\.venv\\lib\\site-packages (from scikit-learn) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\users\\dario\\onedrive\\uu\\courses\\ncml\\project ncml\\hga_feature_selection\\.venv\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in d:\\users\\dario\\onedrive\\uu\\courses\\ncml\\project ncml\\hga_feature_selection\\.venv\\lib\\site-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in d:\\users\\dario\\onedrive\\uu\\courses\\ncml\\project ncml\\hga_feature_selection\\.venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in d:\\users\\dario\\onedrive\\uu\\courses\\ncml\\project ncml\\hga_feature_selection\\.venv\\lib\\site-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\users\\dario\\onedrive\\uu\\courses\\ncml\\project ncml\\hga_feature_selection\\.venv\\lib\\site-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in d:\\users\\dario\\onedrive\\uu\\courses\\ncml\\project ncml\\hga_feature_selection\\.venv\\lib\\site-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in d:\\users\\dario\\onedrive\\uu\\courses\\ncml\\project ncml\\hga_feature_selection\\.venv\\lib\\site-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in d:\\users\\dario\\onedrive\\uu\\courses\\ncml\\project ncml\\hga_feature_selection\\.venv\\lib\\site-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: six>=1.5 in d:\\users\\dario\\onedrive\\uu\\courses\\ncml\\project ncml\\hga_feature_selection\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in d:\\users\\dario\\onedrive\\uu\\courses\\ncml\\project ncml\\hga_feature_selection\\.venv\\lib\\site-packages (from beautifulsoup4->gdown) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in d:\\users\\dario\\onedrive\\uu\\courses\\ncml\\project ncml\\hga_feature_selection\\.venv\\lib\\site-packages (from beautifulsoup4->gdown) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\users\\dario\\onedrive\\uu\\courses\\ncml\\project ncml\\hga_feature_selection\\.venv\\lib\\site-packages (from requests[socks]->gdown) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in d:\\users\\dario\\onedrive\\uu\\courses\\ncml\\project ncml\\hga_feature_selection\\.venv\\lib\\site-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\users\\dario\\onedrive\\uu\\courses\\ncml\\project ncml\\hga_feature_selection\\.venv\\lib\\site-packages (from requests[socks]->gdown) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in d:\\users\\dario\\onedrive\\uu\\courses\\ncml\\project ncml\\hga_feature_selection\\.venv\\lib\\site-packages (from requests[socks]->gdown) (2025.4.26)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in d:\\users\\dario\\onedrive\\uu\\courses\\ncml\\project ncml\\hga_feature_selection\\.venv\\lib\\site-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Requirement already satisfied: colorama in d:\\users\\dario\\onedrive\\uu\\courses\\ncml\\project ncml\\hga_feature_selection\\.venv\\lib\\site-packages (from tqdm->gdown) (0.4.6)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# --- 0. Install necessary library ---\n",
        "#changed as recommended approach (!pip)-->(%pip) which equals to: python -m pip install <module>\n",
        "%pip install gdown pandas numpy scikit-learn matplotlib seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "m8pP3LbUwO38"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler # Keep for potential future use\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import gdown # Library to download from Google Drive\n",
        "from sklearn.impute import SimpleImputer # Import imputer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szfFNuoUwO1G",
        "outputId": "f8e32269-12d5-4b4c-9c05-47af3ea7f35e"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --- 1. Download Data from Google Drive ---\n",
        "file_id = '1dRE9RSdR3pCnDbr5iYHfH2sdR4hOdG3H'\n",
        "output_file = 'synthetic_data_loaded_with_header.csv' # Changed filename for clarity\n",
        "url = f'https://drive.google.com/uc?id={file_id}'\n",
        "\n",
        "print(f\"Downloading file from Google Drive (ID: {file_id})...\")\n",
        "try:\n",
        "    # Using fuzzy=True might help if the direct download link changes slightly\n",
        "    gdown.download(url, output_file, quiet=False, fuzzy=True)\n",
        "    print(f\"File saved as {output_file}\")\n",
        "except Exception as e:\n",
        "    print(f\"Failed to download file: {e}\")\n",
        "    exit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attempting to open the combined file SECOM_combined_dataset.csv saved locally in the folder dataset\n",
            "File successfully opened.\n"
          ]
        }
      ],
      "source": [
        "#Alternativelly use the combined SECOM dataset csv in the dataset subfolder\n",
        "# Define the dataset folder path (the files are in a subfolder called dataset)\n",
        "from pathlib import Path\n",
        "\n",
        "dataset_folder = Path('dataset')\n",
        "dataset_filename= 'SECOM_combined_dataset.csv'\n",
        "\n",
        "print(f\"Attempting to open the combined file {dataset_filename} saved locally in the folder {dataset_folder}\")\n",
        "try:\n",
        "     output_file = dataset_folder / dataset_filename\n",
        "     print(f'File successfully opened.')\n",
        "except FileNotFoundError as e:\n",
        "     print(f'File not found: {e}')\n",
        "except Exception as e:\n",
        "     print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHfX4xZuwOxq",
        "outputId": "5f236e2a-16f0-4e78-962e-f246052d4606"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data loaded successfully from dataset\\SECOM_combined_dataset.csv with shape: (1567, 591)\n",
            "\n",
            "First 5 rows of loaded data:\n",
            "   feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
            "0    3030.93    2564.00  2187.7333  1411.1265     1.3602      100.0   \n",
            "1    3095.78    2465.14  2230.4222  1463.6606     0.8294      100.0   \n",
            "2    2932.61    2559.94  2186.4111  1698.0172     1.5102      100.0   \n",
            "3    2988.72    2479.90  2199.0333   909.7926     1.3204      100.0   \n",
            "4    3032.24    2502.87  2233.3667  1326.5200     1.5334      100.0   \n",
            "\n",
            "   feature_6  feature_7  feature_8  feature_9  ...  feature_581  feature_582  \\\n",
            "0    97.6133     0.1242     1.5005     0.0162  ...          NaN       0.5005   \n",
            "1   102.3433     0.1247     1.4966    -0.0005  ...     208.2045       0.5019   \n",
            "2    95.4878     0.1241     1.4436     0.0041  ...      82.8602       0.4958   \n",
            "3   104.2367     0.1217     1.4882    -0.0124  ...      73.8432       0.4990   \n",
            "4   100.3967     0.1235     1.5031    -0.0031  ...          NaN       0.4800   \n",
            "\n",
            "   feature_583  feature_584  feature_585  feature_586  feature_587  \\\n",
            "0       0.0118       0.0035       2.3630          NaN          NaN   \n",
            "1       0.0223       0.0055       4.4447       0.0096       0.0201   \n",
            "2       0.0157       0.0039       3.1745       0.0584       0.0484   \n",
            "3       0.0103       0.0025       2.0544       0.0202       0.0149   \n",
            "4       0.4766       0.1045      99.3032       0.0202       0.0149   \n",
            "\n",
            "   feature_588  feature_589  label  \n",
            "0          NaN          NaN     -1  \n",
            "1       0.0060     208.2045     -1  \n",
            "2       0.0148      82.8602      1  \n",
            "3       0.0044      73.8432     -1  \n",
            "4       0.0044      73.8432     -1  \n",
            "\n",
            "[5 rows x 591 columns]\n",
            "\n",
            "Data info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1567 entries, 0 to 1566\n",
            "Columns: 591 entries, feature_0 to label\n",
            "dtypes: float64(590), int64(1)\n",
            "memory usage: 7.1 MB\n"
          ]
        }
      ],
      "source": [
        "# --- 2. Load Data using Pandas (Assuming Header IS Present) ---\n",
        "try:\n",
        "    # FIX: Remove header=None to let Pandas read the first row as header\n",
        "    data = pd.read_csv(output_file)\n",
        "    print(f\"Data loaded successfully from {output_file} with shape: {data.shape}\")\n",
        "    # Display first few rows and info to check data types\n",
        "    print(\"\\nFirst 5 rows of loaded data:\")\n",
        "    print(data.head())\n",
        "    print(\"\\nData info:\")\n",
        "    data.info()\n",
        "except Exception as e:\n",
        "    print(f\"Failed to load data from {output_file}: {e}\")\n",
        "    exit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jn6x0zd5wOu5",
        "outputId": "2ab6778d-5717-4a80-99d4-ec829d0b7389"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Feature names extracted from header: ['feature_0', 'feature_1', 'feature_2', 'feature_3', 'feature_4']... (Total: 590)\n",
            "Features shape after cleaning: (1567, 590)\n",
            "Number of missing labels to be imputed: 0\n",
            "Target shape after cleaning: (1567,)\n"
          ]
        }
      ],
      "source": [
        "# --- 3. Prepare Data ---\n",
        "if data.shape[1] > 1:\n",
        "    # Assume last column is target 'y', all others are features 'X'\n",
        "    feature_names = data.columns[:-1].tolist() # Get actual feature names from header\n",
        "    X_df = data[feature_names] # Select feature columns using names\n",
        "    y_series = data.iloc[:, -1] # Select target column by position\n",
        "\n",
        "    print(f\"\\nFeature names extracted from header: {feature_names[:5]}... (Total: {len(feature_names)})\")\n",
        "\n",
        "    # --- Keep Robust Cleaning Steps ---\n",
        "    # Convert non-numeric strings to numeric, coercing errors, otherwise-> NaN\n",
        "    X_df_numeric = X_df.apply(pd.to_numeric, errors='coerce')\n",
        "    y_series_numeric = pd.to_numeric(y_series, errors='coerce')\n",
        "\n",
        "    # Handle potential NaN values resulting from coercion or missing values in original file\n",
        "    imputer_X = SimpleImputer(strategy='mean')\n",
        "    X_imputed = imputer_X.fit_transform(X_df_numeric)\n",
        "\n",
        "    missing_labels_count = y_series_numeric.isna().sum()\n",
        "    \n",
        "    imputer_y = SimpleImputer(strategy='most_frequent') #changed strategy from mean to most frequent since we have binary labels\n",
        "    y_imputed = imputer_y.fit_transform(y_series_numeric.values.reshape(-1, 1)).flatten()\n",
        "\n",
        "    # Check for NaNs after imputation (should only happen if a whole column was non-numeric/NaN)\n",
        "    if np.isnan(X_imputed).any() or np.isnan(y_imputed).any():\n",
        "        print(\"Warning: NaNs still present after imputation. Check columns with all invalid values.\")\n",
        "        # Consider more advanced imputation or dropping problematic columns/rows if this occurs\n",
        "\n",
        "    X = X_imputed\n",
        "    y = y_imputed\n",
        "\n",
        "    print(f\"Features shape after cleaning: {X.shape}\")\n",
        "    \n",
        "    print(f\"Number of missing labels to be imputed: {missing_labels_count}\")\n",
        "    print(f\"Target shape after cleaning: {y.shape}\")\n",
        "    n_features_loaded = X.shape[1] # Keep track of the number of features\n",
        "\n",
        "else:\n",
        "    print(\"Error: Loaded data has only one column. Cannot separate features and target.\")\n",
        "    exit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-goCXrEowOrh",
        "outputId": "c1782c36-b773-442e-eee8-274b2a701ad0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training features shape: (1096, 590)\n",
            "Validation features shape: (471, 590)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# --- 4. Split Data for Fitness Evaluation ---\n",
        "# Using the cleaned X and y, with stratify helps maintain the proportion of class labels\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "n_features = X_train.shape[1] # Use the actual number of features\n",
        "print(f\"\\nTraining features shape: {X_train.shape}\")\n",
        "print(f\"Validation features shape: {X_val.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Fitness Function (using Random Forest) ---\n",
        "def fitness_function(individual, model, penalty):\n",
        "    selected_indices = np.where(individual == 1)[0] # Get indices where bit is 1\n",
        "    num_selected = len(selected_indices)\n",
        "\n",
        "    #TODO: This validation can be done before calling this function!!!!!!!!!\n",
        "    if num_selected == 0:\n",
        "        return -np.inf # Heavily penalize empty feature set\n",
        "\n",
        "    # Select corresponding columns from train/validation sets\n",
        "    X_train_sel = X_train[:, selected_indices]\n",
        "    X_val_sel = X_val[:, selected_indices]\n",
        "\n",
        "    # Check for NaNs *before* fitting (added robustness)\n",
        "    \"\"\" if np.isnan(X_train_sel).any() or np.isnan(y_train).any() or np.isnan(X_val_sel).any():\n",
        "         # This case should ideally not happen after imputation, but good safeguard\n",
        "         print(\"Warning: NaN detected in data subset for fitness evaluation.\")\n",
        "         return -np.inf # Penalize subsets leading to NaNs \"\"\"\n",
        "    \n",
        "    try:        \n",
        "        model.fit(X_train_sel, y_train)\n",
        "        y_pred = model.predict(X_val_sel)\n",
        "        mse = mean_squared_error(y_val, y_pred)\n",
        "        # Fitness = Negative MSE (maximization) - Penalty for feature count\n",
        "        fitness = -mse - penalty * num_selected\n",
        "    except ValueError as e:\n",
        "        # Catch potential errors during fitting/prediction if data issues remain\n",
        "        print(f\"Error during model fitting/prediction: {e}\")\n",
        "        fitness = -np.inf # Penalize if model fails\n",
        "    return fitness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "from joblib import Parallel, delayed\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "def evaluate_population(population, classif_model, penalty, n_jobs=-1, batch_size=10):\n",
        "    print(f'\\nEvaluating population fitness in parallel (batch_size={batch_size})..')\n",
        "\n",
        "    # Wrapper to enable tqdm progress bar\n",
        "    results = Parallel(n_jobs=n_jobs, batch_size=batch_size)(\n",
        "        delayed(fitness_function)(ind, classif_model, penalty) \n",
        "        for ind in tqdm(population, desc=\"Fitness Evaluation\")\n",
        "    )\n",
        "\n",
        "    return np.array(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "#The expected number of selected features is d ⊆ n_features, this constraint must be set up since the first population\n",
        "# to make more likely for the algorithm to focus on small subsets instead of larger ones in further generations.\n",
        "#We set the chance of each gene being 1 in the initial population P to a value of d/D\n",
        "def initialize_population(d, pop_size, n_features):\n",
        "    probability_for_1 = d/n_features\n",
        "    probability_for_0 = 1 - probability_for_1\n",
        "    \n",
        "    print(f'-Building Initial population of size: {pop_size} with approximately {d} features per individual')\n",
        "    \n",
        "    pop = np.random.choice([0, 1], size=(pop_size, n_features), p=[probability_for_0, probability_for_1])    \n",
        "    \n",
        "    # Ensure initial individuals are not all zeros\n",
        "    for i in range(pop_size):\n",
        "        if not pop[i].any:\n",
        "            #then randomly sets one feature to 1.\n",
        "            pop[i, np.random.randint(0, n_features)] = 1\n",
        "    return pop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tournament_selection(pop, fitnesses, k=3):\n",
        "    # Handle potential -inf fitness values gracefully\n",
        "    valid_indices = np.where(np.isfinite(fitnesses))[0]\n",
        "    if len(valid_indices) == 0:\n",
        "        # If no individuals have valid fitness, return random individuals to avoid crash\n",
        "        print(\"Warning: No individuals with valid fitness in selection.\")\n",
        "        return pop[np.random.choice(len(pop), size=len(pop))]\n",
        "    if len(valid_indices) < k:\n",
        "        # If fewer valid individuals than tournament size, select randomly from valid ones\n",
        "        selected_idx = np.random.choice(valid_indices, size=len(pop), replace=True) # Allow replacement\n",
        "        return pop[selected_idx]\n",
        "\n",
        "    selected = []\n",
        "    for _ in range(len(pop)):\n",
        "       # Ensure tournament participants are chosen only from valid individuals\n",
        "       tournament_contenders_indices = np.random.choice(valid_indices, k, replace=False)\n",
        "       winner_idx_in_contenders = np.argmax(fitnesses[tournament_contenders_indices])\n",
        "       winner_original_idx = tournament_contenders_indices[winner_idx_in_contenders]\n",
        "       selected.append(pop[winner_original_idx])\n",
        "    return np.array(selected)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def uniform_crossover(parent1, parent2):\n",
        "    mask = np.random.randint(0, 2, size=len(parent1), dtype=bool)\n",
        "    child = np.where(mask, parent1, parent2)\n",
        "    return child\n",
        "\n",
        "def mutate(individual, mutation_rate):\n",
        "    mutant = individual.copy()\n",
        "    mutation_mask = np.random.rand(len(mutant)) < mutation_rate\n",
        "    mutant[mutation_mask] = 1 - mutant[mutation_mask] # Flip bits\n",
        "    # Ensure mutation doesn't result in an all-zero individual\n",
        "    if np.sum(mutant) == 0 and len(mutant) > 0:\n",
        "         mutant[np.random.randint(0, len(mutant))] = 1\n",
        "    return mutant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Local Search ---\n",
        "# (This function remains unchanged structurally)\n",
        "def local_search(individual, fitness_func, steps, model, penalty): # Uses tuned steps\n",
        "    best = individual.copy()\n",
        "    current_fitness = fitness_func(best, model, penalty)\n",
        "    # Cannot perform local search on invalid individual\n",
        "    if not np.isfinite(current_fitness):\n",
        "        return best\n",
        "\n",
        "    indices_to_try = np.arange(len(best))\n",
        "    for _ in range(steps):\n",
        "        np.random.shuffle(indices_to_try)\n",
        "        improved_in_step = False\n",
        "        for idx in indices_to_try:\n",
        "            candidate = best.copy()\n",
        "            candidate[idx] = 1 - candidate[idx] # Flip bit\n",
        "            # Ensure candidate is not all zeros\n",
        "            if np.sum(candidate) == 0:\n",
        "                continue # Skip invalid (all zero) candidate\n",
        "            candidate_fitness = fitness_func(candidate, model, penalty)\n",
        "            # Only accept improvement if candidate_fitness is valid and better\n",
        "            if np.isfinite(candidate_fitness) and candidate_fitness > current_fitness:\n",
        "                best = candidate\n",
        "                current_fitness = candidate_fitness\n",
        "                improved_in_step = True\n",
        "                break # Move to next step once an improvement is found (first improvement)\n",
        "        if not improved_in_step:\n",
        "            break # Stop if no single flip improved fitness in a full pass\n",
        "    return best"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "qxQhWy3awObl"
      },
      "outputs": [],
      "source": [
        "# --- 5. Hybrid Genetic Algorithm for Feature Selection ---\n",
        "# --- Main HGA Loop ---\n",
        "# (This function remains unchanged structurally, but uses the tuned parameters)\n",
        "#d_features: Desired features\n",
        "def hybrid_ga_feature_selection(d_features, pop_size, n_gen, classif_model, penalty, elitism, n_elite, crossover_rate, mutation_rate,\n",
        "                                 ls_top_k, local_search_steps):\n",
        "    population = initialize_population(d_features, pop_size, n_features)\n",
        "    best_fitnesses = []\n",
        "    best_individual_overall = None\n",
        "    best_fit_overall = -np.inf\n",
        "    features_selected_history = []\n",
        "\n",
        "    print(\"\\nStarting Hybrid Genetic Algorithm...\")\n",
        "    for gen in range(n_gen): # Uses tuned N_GEN\n",
        "        print(f'\\n-Running Generation: {gen}----')\n",
        "        \n",
        "        # Evaluate fitness for the current population\n",
        "        print(f'\\nEvaluating population fitness..')\n",
        "        #fitnesses = np.array([fitness_function(ind, classif_model, penalty) for ind in population])          \n",
        "        fitnesses = evaluate_population(population, classif_model, penalty, n_jobs=-1, batch_size=10)      \n",
        "\n",
        "        # Find best valid individual in generation\n",
        "        print(f'\\nFinding best valid individual..')\n",
        "\n",
        "        valid_fitness_indices = np.where(np.isfinite(fitnesses))[0]\n",
        "        if len(valid_fitness_indices) == 0:\n",
        "            print(f\"Warning: No valid individuals in generation {gen+1}. Stopping early.\")\n",
        "            # Keep the last known best if available, otherwise end with None\n",
        "            best_fitnesses.append(best_fit_overall if np.isfinite(best_fit_overall) else np.nan)\n",
        "            features_selected_history.append(np.sum(best_individual_overall) if best_individual_overall is not None else 0)\n",
        "            break # Stop the GA run\n",
        "\n",
        "        gen_best_idx_among_valid = np.argmax(fitnesses[valid_fitness_indices])\n",
        "        gen_best_original_idx = valid_fitness_indices[gen_best_idx_among_valid]\n",
        "        gen_best_fitness = fitnesses[gen_best_original_idx]\n",
        "\n",
        "        # Update overall best\n",
        "        if gen_best_fitness > best_fit_overall:\n",
        "            best_fit_overall = gen_best_fitness\n",
        "            best_individual_overall = population[gen_best_original_idx].copy()\n",
        "\n",
        "        # Store history based on overall best\n",
        "        best_fitnesses.append(best_fit_overall)\n",
        "        features_selected_history.append(np.sum(best_individual_overall) if best_individual_overall is not None else 0)\n",
        "\n",
        "        #--- GA Operators (Selection, Crossover, Mutation) ---\n",
        "        # (These functions remain unchanged structurally)\n",
        "        # Elitism\n",
        "        if elitism:\n",
        "             print(f'Aplying Elitism..')\n",
        "             sorted_valid_indices = valid_fitness_indices[np.argsort(fitnesses[valid_fitness_indices])]\n",
        "             elite_indices = sorted_valid_indices[-n_elite:]\n",
        "             elites = population[elite_indices].copy()\n",
        "        else:\n",
        "             elites = np.empty((0, n_features), dtype=int)\n",
        "\n",
        "        # Selection\n",
        "        print(f'Selecting parents by {tournament_selection.__name__}...')\n",
        "        selected_parents = tournament_selection(population, fitnesses)\n",
        "\n",
        "        \n",
        "        # Crossover & Mutation\n",
        "        print(f'Applying Crossover and Mutation operators...')\n",
        "        offspring = np.empty_like(population)\n",
        "        for i in range(0, pop_size, 2): # Uses tuned POP_SIZE\n",
        "            p1_idx, p2_idx = i, (i + 1) % pop_size\n",
        "            parent1, parent2 = selected_parents[p1_idx], selected_parents[p2_idx]\n",
        "            if np.random.rand() < crossover_rate:\n",
        "                child1 = uniform_crossover(parent1, parent2)\n",
        "                child2 = uniform_crossover(parent2, parent1)\n",
        "            else:\n",
        "                child1, child2 = parent1.copy(), parent2.copy()\n",
        "            offspring[i] = mutate(child1, mutation_rate)\n",
        "            if i + 1 < pop_size:\n",
        "                offspring[i+1] = mutate(child2, mutation_rate)\n",
        "\n",
        "        # Local Search on top K individuals (uses tuned LS_TOP_K)\n",
        "        #fitnesses_offspring_pre_ls = np.array([fitness_function(ind, classif_model, penalty) for ind in offspring])\n",
        "        fitnesses_offspring_pre_ls = evaluate_population(offspring, classif_model, penalty, n_jobs=-1, batch_size=10)      \n",
        "        \n",
        "        valid_offspring_indices = np.where(np.isfinite(fitnesses_offspring_pre_ls))[0]\n",
        "\n",
        "        if len(valid_offspring_indices) > 0:\n",
        "             sorted_valid_offspring_indices = valid_offspring_indices[np.argsort(fitnesses_offspring_pre_ls[valid_offspring_indices])]\n",
        "             top_indices_for_ls = sorted_valid_offspring_indices[-ls_top_k:] # Uses tuned LS_TOP_K\n",
        "             print(f'Applying Local Search...')\n",
        "             for idx in top_indices_for_ls:\n",
        "                 # Uses tuned LOCAL_SEARCH_STEPS inside the function call\n",
        "                 offspring[idx] = local_search(offspring[idx], fitness_function, local_search_steps, classif_model, penalty) \n",
        "\n",
        "        # Replacement with Elitism        \n",
        "        if elitism and len(elites) > 0:\n",
        "            print(f'Obtaining offspring with Elitism...')\n",
        "            #fitnesses_offspring_post_ls = np.array([fitness_function(ind, classif_model, penalty) for ind in offspring])\n",
        "            fitnesses_offspring_post_ls = evaluate_population(offspring, classif_model, penalty, n_jobs=-1, batch_size=10)      \n",
        "\n",
        "            valid_offspring_indices_post_ls = np.where(np.isfinite(fitnesses_offspring_post_ls))[0]\n",
        "            if len(valid_offspring_indices_post_ls) >= n_elite:\n",
        "                sorted_valid_offspring_indices_post_ls = valid_offspring_indices_post_ls[np.argsort(fitnesses_offspring_post_ls[valid_offspring_indices_post_ls])]\n",
        "                worst_indices = sorted_valid_offspring_indices_post_ls[:n_elite]\n",
        "                num_to_replace = min(len(worst_indices), len(elites))\n",
        "                offspring[worst_indices[:num_to_replace]] = elites[:num_to_replace]\n",
        "\n",
        "        population = offspring\n",
        "\n",
        "        current_best_features = np.sum(best_individual_overall) if best_individual_overall is not None else 0\n",
        "        print(f\"Generation {gen+1}/{n_gen}: Best Fitness = {best_fit_overall:.4f}, Features Selected = {current_best_features}\")\n",
        "\n",
        "    print(\"Hybrid Genetic Algorithm finished.\")\n",
        "    return best_individual_overall, best_fitnesses, features_selected_history\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-SDv6h_wjlE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zj3nYqt40ovq",
        "outputId": "7ee3ea7b-a0bc-44c2-fba8-e107ae167d43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-Building Initial population of size: 100 with approximately 50 features per individual\n",
            "\n",
            "Starting Hybrid Genetic Algorithm...\n",
            "\n",
            "-Running Generation: 0----\n",
            "\n",
            "Evaluating population fitness..\n",
            "\n",
            "Evaluating population fitness in parallel (batch_size=10)..\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fitness Evaluation: 100%|██████████| 100/100 [00:04<00:00, 24.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Finding best valid individual..\n",
            "Aplying Elitism..\n",
            "Selecting parents by tournament_selection...\n",
            "Applying Crossover and Mutation operators...\n",
            "\n",
            "Evaluating population fitness in parallel (batch_size=10)..\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fitness Evaluation: 100%|██████████| 100/100 [00:03<00:00, 32.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Applying Local Search...\n",
            "Obtaining offspring with Elitism...\n",
            "\n",
            "Evaluating population fitness in parallel (batch_size=10)..\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fitness Evaluation: 100%|██████████| 100/100 [00:11<00:00,  8.97it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generation 1/20: Best Fitness = -0.5854, Features Selected = 31\n",
            "\n",
            "-Running Generation: 1----\n",
            "\n",
            "Evaluating population fitness..\n",
            "\n",
            "Evaluating population fitness in parallel (batch_size=10)..\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fitness Evaluation: 100%|██████████| 100/100 [00:08<00:00, 11.19it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Finding best valid individual..\n",
            "Aplying Elitism..\n",
            "Selecting parents by tournament_selection...\n",
            "Applying Crossover and Mutation operators...\n",
            "\n",
            "Evaluating population fitness in parallel (batch_size=10)..\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fitness Evaluation: 100%|██████████| 100/100 [00:09<00:00, 10.47it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Applying Local Search...\n",
            "Obtaining offspring with Elitism...\n",
            "\n",
            "Evaluating population fitness in parallel (batch_size=10)..\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fitness Evaluation: 100%|██████████| 100/100 [00:09<00:00, 10.15it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generation 2/20: Best Fitness = -0.5854, Features Selected = 31\n",
            "\n",
            "-Running Generation: 2----\n",
            "\n",
            "Evaluating population fitness..\n",
            "\n",
            "Evaluating population fitness in parallel (batch_size=10)..\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fitness Evaluation: 100%|██████████| 100/100 [00:09<00:00, 10.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Finding best valid individual..\n",
            "Aplying Elitism..\n",
            "Selecting parents by tournament_selection...\n",
            "Applying Crossover and Mutation operators...\n",
            "\n",
            "Evaluating population fitness in parallel (batch_size=10)..\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fitness Evaluation: 100%|██████████| 100/100 [00:09<00:00, 10.43it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Applying Local Search...\n",
            "Obtaining offspring with Elitism...\n",
            "\n",
            "Evaluating population fitness in parallel (batch_size=10)..\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fitness Evaluation: 100%|██████████| 100/100 [00:10<00:00,  9.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generation 3/20: Best Fitness = -0.5854, Features Selected = 31\n",
            "\n",
            "-Running Generation: 3----\n",
            "\n",
            "Evaluating population fitness..\n",
            "\n",
            "Evaluating population fitness in parallel (batch_size=10)..\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fitness Evaluation: 100%|██████████| 100/100 [00:08<00:00, 11.36it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Finding best valid individual..\n",
            "Aplying Elitism..\n",
            "Selecting parents by tournament_selection...\n",
            "Applying Crossover and Mutation operators...\n",
            "\n",
            "Evaluating population fitness in parallel (batch_size=10)..\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fitness Evaluation: 100%|██████████| 100/100 [00:12<00:00,  7.83it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Applying Local Search...\n",
            "Obtaining offspring with Elitism...\n",
            "\n",
            "Evaluating population fitness in parallel (batch_size=10)..\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fitness Evaluation: 100%|██████████| 100/100 [00:11<00:00,  8.76it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generation 4/20: Best Fitness = -0.5854, Features Selected = 31\n",
            "\n",
            "-Running Generation: 4----\n",
            "\n",
            "Evaluating population fitness..\n",
            "\n",
            "Evaluating population fitness in parallel (batch_size=10)..\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fitness Evaluation: 100%|██████████| 100/100 [00:10<00:00,  9.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Finding best valid individual..\n",
            "Aplying Elitism..\n",
            "Selecting parents by tournament_selection...\n",
            "Applying Crossover and Mutation operators...\n",
            "\n",
            "Evaluating population fitness in parallel (batch_size=10)..\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fitness Evaluation: 100%|██████████| 100/100 [00:13<00:00,  7.43it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Applying Local Search...\n",
            "Obtaining offspring with Elitism...\n",
            "\n",
            "Evaluating population fitness in parallel (batch_size=10)..\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fitness Evaluation: 100%|██████████| 100/100 [00:13<00:00,  7.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generation 5/20: Best Fitness = -0.5854, Features Selected = 31\n",
            "\n",
            "-Running Generation: 5----\n",
            "\n",
            "Evaluating population fitness..\n",
            "\n",
            "Evaluating population fitness in parallel (batch_size=10)..\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fitness Evaluation: 100%|██████████| 100/100 [00:16<00:00,  6.13it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Finding best valid individual..\n",
            "Aplying Elitism..\n",
            "Selecting parents by tournament_selection...\n",
            "Applying Crossover and Mutation operators...\n",
            "\n",
            "Evaluating population fitness in parallel (batch_size=10)..\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fitness Evaluation:   0%|          | 0/100 [00:00<?, ?it/s]"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     21\u001b[39m model = RandomForestRegressor(n_estimators=\u001b[32m15\u001b[39m, \u001b[38;5;66;03m# Reduced from 50\u001b[39;00m\n\u001b[32m     22\u001b[39m                                 random_state=\u001b[32m42\u001b[39m,\n\u001b[32m     23\u001b[39m                                 n_jobs=-\u001b[32m1\u001b[39m) \u001b[38;5;66;03m# Use n_jobs=-1 for parallelization\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# --- TUNING POINT ---\u001b[39;00m\n\u001b[32m     25\u001b[39m \n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# --- 6. Run the Hybrid GA ---\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m best_solution, fitness_history, features_selected_history = \u001b[43mhybrid_ga_feature_selection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDESIRED_FEATURES_NUMBER\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPOP_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_GEN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPENALTY_COEFF\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mELITISM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_ELITE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCROSSOVER_RATE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMUTATION_RATE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m                                 \u001b[49m\u001b[43mLS_TOP_K\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLOCAL_SEARCH_STEPS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# --- 7. Results ---\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m best_solution \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 79\u001b[39m, in \u001b[36mhybrid_ga_feature_selection\u001b[39m\u001b[34m(d_features, pop_size, n_gen, classif_model, penalty, elitism, n_elite, crossover_rate, mutation_rate, ls_top_k, local_search_steps)\u001b[39m\n\u001b[32m     75\u001b[39m         offspring[i+\u001b[32m1\u001b[39m] = mutate(child2, mutation_rate)\n\u001b[32m     77\u001b[39m \u001b[38;5;66;03m# Local Search on top K individuals (uses tuned LS_TOP_K)\u001b[39;00m\n\u001b[32m     78\u001b[39m \u001b[38;5;66;03m#fitnesses_offspring_pre_ls = np.array([fitness_function(ind, classif_model, penalty) for ind in offspring])\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m fitnesses_offspring_pre_ls = \u001b[43mevaluate_population\u001b[49m\u001b[43m(\u001b[49m\u001b[43moffspring\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassif_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpenalty\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m      \n\u001b[32m     81\u001b[39m valid_offspring_indices = np.where(np.isfinite(fitnesses_offspring_pre_ls))[\u001b[32m0\u001b[39m]\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(valid_offspring_indices) > \u001b[32m0\u001b[39m:\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mevaluate_population\u001b[39m\u001b[34m(population, classif_model, penalty, n_jobs, batch_size)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEvaluating population fitness in parallel (batch_size=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)..\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Wrapper to enable tqdm progress bar\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m results = \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfitness_function\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassif_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpenalty\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mind\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpopulation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mFitness Evaluation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m np.array(results)\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\Dario\\OneDrive\\UU\\Courses\\NCML\\Project NCML\\HGA_feature_selection\\.venv\\Lib\\site-packages\\joblib\\parallel.py:2071\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2065\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2071\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\Dario\\OneDrive\\UU\\Courses\\NCML\\Project NCML\\HGA_feature_selection\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1681\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1678\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1680\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1681\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1683\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1684\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1687\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\Dario\\OneDrive\\UU\\Courses\\NCML\\Project NCML\\HGA_feature_selection\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1799\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1788\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_ordered:\n\u001b[32m   1789\u001b[39m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[32m   1790\u001b[39m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1794\u001b[39m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[32m   1795\u001b[39m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[32m   1796\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1797\u001b[39m         \u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING\n\u001b[32m   1798\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m1799\u001b[39m         \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1800\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1802\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs == \u001b[32m0\u001b[39m:\n\u001b[32m   1803\u001b[39m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[32m   1804\u001b[39m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1810\u001b[39m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[32m   1811\u001b[39m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# GA Parameters (TUNED for potentially faster runtime)\n",
        "# --- TUNING POINT ---\n",
        "POP_SIZE = 100 # Reduced from 30\n",
        "N_GEN = 20    # Reduced from 30\n",
        "# --- TUNING POINT ---\n",
        "CROSSOVER_RATE = 0.8 # Kept the same\n",
        "MUTATION_RATE = 0.1  # Kept the same\n",
        "ELITISM = True       # Kept the same\n",
        "N_ELITE = 1          # Kept the same\n",
        "# --- TUNING POINT ---\n",
        "LOCAL_SEARCH_STEPS = 2 # Reduced from 5\n",
        "LS_TOP_K = 2          # Reduced from 3\n",
        "# --- TUNING POINT ---\n",
        "PENALTY_COEFF = 0.01   # Kept the same\n",
        "\n",
        "DESIRED_FEATURES_NUMBER = 50\n",
        "\n",
        "# DEfine the Classifier model: Random Forest\n",
        "# --- TUNING POINT ---\n",
        "# Use fewer estimators for significantly faster fitness evaluation\n",
        "model = RandomForestRegressor(n_estimators=15, # Reduced from 50\n",
        "                                random_state=42,\n",
        "                                n_jobs=-1) # Use n_jobs=-1 for parallelization\n",
        "# --- TUNING POINT ---\n",
        "\n",
        "# --- 6. Run the Hybrid GA ---\n",
        "best_solution, fitness_history, features_selected_history = hybrid_ga_feature_selection(DESIRED_FEATURES_NUMBER, POP_SIZE, N_GEN, model, PENALTY_COEFF, ELITISM, N_ELITE, CROSSOVER_RATE, MUTATION_RATE,\n",
        "                                 LS_TOP_K, LOCAL_SEARCH_STEPS)\n",
        "\n",
        "# --- 7. Results ---\n",
        "if best_solution is not None:\n",
        "    selected_feature_indices = np.where(best_solution == 1)[0]\n",
        "    print(\"\\n--- HGA Results ---\")\n",
        "    print(\"Best feature subset found:\")\n",
        "    print(\"Selected feature indices:\", selected_feature_indices)\n",
        "    # Use ACTUAL feature names from the header\n",
        "    selected_names = [feature_names[i] for i in selected_feature_indices]\n",
        "    print(\"Selected feature names:\", selected_names)\n",
        "    print(\"Number of features selected:\", len(selected_feature_indices))\n",
        "    print(f\"Final Best Fitness: {fitness_history[-1]:.4f}\")\n",
        "else:\n",
        "    print(\"\\nNo solution found or algorithm stopped early.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Av017jN2wpW3"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --- 8. Plot Fitness Curve and Number of Features Selected ---\n",
        "if best_solution is not None and fitness_history and features_selected_history:\n",
        "    generations = range(1, len(fitness_history) + 1)\n",
        "    plt.figure(figsize=(14, 6))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(generations, fitness_history, marker='o', linestyle='-')\n",
        "    plt.xlabel('Generation')\n",
        "    plt.ylabel('Best Fitness')\n",
        "    plt.title('HGA Convergence: Best Fitness per Generation')\n",
        "    plt.grid(True)\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(generations, features_selected_history, marker='s', linestyle='-', color='orange')\n",
        "    plt.xlabel('Generation')\n",
        "    plt.ylabel('Number of Features Selected')\n",
        "    plt.title('HGA: Number of Features in Best Solution per Generation')\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Cannot plot results as no valid solution was found or history is empty.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-tDANHJwLd_"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
